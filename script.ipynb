{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK are divided based on the provided structure in\n",
    " https://docs.google.com/document/d/e/2PACX-1vQfC8gkrSx_ycYkIOdae5sJ-fuqn2UA9nLtGqA5egBuwNKMNZpi_NBR0MRnnqdWt8WYqznE6x9_DIO0/pub**\n",
    "\n",
    "These are scripts although they are also provided as separate files in script folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK-1\n",
    "\n",
    "**Extract Reddit data**\n",
    "Then Apply PreProcessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your CLIENT_ID and CLIENT_SECRET in .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "client_id = os.getenv(\"CLIENT_ID\")\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "class RedditMentalHealthScraper:\n",
    "    def __init__(self, client_id, client_secret, user_agent=\"SocialAnalysis<v0>\"):\n",
    "        \"\"\"\n",
    "        Initialize Reddit API connection\n",
    "        \"\"\"\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "        \n",
    "        self.keywords = [\n",
    "            # Mental Health\n",
    "            \"depressed\", \"anxiety\", \"mental health\", \"ptsd\", \"trauma\", \n",
    "            \"burnout\", \"emotional support\", \"mental breakdown\", \n",
    "            \"psychological distress\", \"intrusive thoughts\",\n",
    "            \n",
    "            # Substance Use\n",
    "            \"addiction\", \"substance abuse\", \"recovery\", \"relapse\", \n",
    "            \"alcohol addiction\", \"drug addiction\", \"sober\", \n",
    "            \"addiction help\", \"substance use disorder\",\n",
    "            \n",
    "            # Emotional Distress\n",
    "            \"overwhelmed\", \"struggling\", \"feeling hopeless\", \n",
    "            \"emotional pain\", \"self-harm\", \"suicidal thoughts\", \n",
    "            \"suicide prevention\", \"mental health crisis\",\n",
    "            \n",
    "            # Treatment and Support\n",
    "            \"therapy\", \"counseling\", \"medication\", \"support group\", \n",
    "            \"mental health resources\", \"coping mechanisms\"\n",
    "        ]\n",
    "    \n",
    "    def extract_posts(self, target_subreddits, limit=100):\n",
    "        \"\"\"\n",
    "        Extract posts from subreddits containing target keywords\n",
    "        \"\"\"\n",
    "        extracted_posts = []\n",
    "        \n",
    "        # Find all subreddits matching target names\n",
    "        matching_subreddits = []\n",
    "        for target in target_subreddits:\n",
    "            try:\n",
    "                subreddits = self.reddit.subreddits.search_by_name(target, include_nsfw=True)\n",
    "                for i in subreddits:\n",
    "                    matching_subreddits.append(i.display_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find subreddit matching {target}: {e}\")\n",
    "        \n",
    "        print(f\"Matching subreddits found: {matching_subreddits}\")\n",
    "        for subreddit_name in tqdm(matching_subreddits):\n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                \n",
    "                # Search hot posts and new posts\n",
    "                for post_stream in [subreddit.hot(limit=limit), subreddit.new(limit=limit)]:\n",
    "                    for post in post_stream:\n",
    "                        # Check if post contains any of the keywords\n",
    "                        if self._contains_keywords(post.title.lower()) or \\\n",
    "                           self._contains_keywords(post.selftext.lower()):\n",
    "                            \n",
    "                            post_data = {\n",
    "                                'post_id': post.id,\n",
    "                                'timestamp': datetime.fromtimestamp(post.created_utc).isoformat(),\n",
    "                                'title': post.title,\n",
    "                                'content': post.selftext,\n",
    "                                'subreddit': subreddit_name,\n",
    "                                'ups': post.ups,\n",
    "                                'num_comments': post.num_comments,\n",
    "                                'url': post.url\n",
    "                            }\n",
    "                            \n",
    "                            extracted_posts.append(post_data)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subreddit {subreddit_name}: {e}\")\n",
    "        \n",
    "        return extracted_posts\n",
    "    \n",
    "    # Method istested but not used in analysis, as it needs higher api calls\n",
    "    def extract_posts_with_comments(self, target_subreddits, limit=100, comment_limit=50, replace_more_limit=0):\n",
    "        \"\"\"\n",
    "        Extract posts from subreddits containing target keywords along with their comments\n",
    "        \n",
    "        Parameters:\n",
    "        - target_subreddits: List of subreddit names to search\n",
    "        - limit: Maximum number of posts to retrieve per subreddit\n",
    "        - comment_limit: Maximum number of comments to retrieve per post\n",
    "        \n",
    "        Returns:\n",
    "        - List of dictionaries containing post data and comments\n",
    "        \"\"\"\n",
    "        extracted_posts = []\n",
    "        \n",
    "        # Find all subreddits matching target names\n",
    "        matching_subreddits = []\n",
    "        for target in target_subreddits:\n",
    "            try:\n",
    "                subreddits = self.reddit.subreddits.search_by_name(target, include_nsfw=True)\n",
    "                for i in subreddits:\n",
    "                    matching_subreddits.append(i.display_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find subreddit matching {target}: {e}\")\n",
    "        \n",
    "        print(f\"Matching subreddits found: {matching_subreddits}\")\n",
    "        for subreddit_name in tqdm(matching_subreddits):\n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "                \n",
    "                # Search hot posts and new posts\n",
    "                for post_stream in [subreddit.hot(limit=limit), subreddit.new(limit=limit)]:\n",
    "                    for post in post_stream:\n",
    "                        # Check if post contains any of the keywords\n",
    "                        if self._contains_keywords(post.title.lower()) or \\\n",
    "                        self._contains_keywords(post.selftext.lower()):\n",
    "                            \n",
    "                            # Extract comments\n",
    "                            comments_data = []\n",
    "                            post.comments.replace_more(limit=replace_more_limit)  # Replace MoreComments objects\n",
    "                            \n",
    "                            for comment in post.comments.list()[:comment_limit]:\n",
    "                                comment_data = {\n",
    "                                    'comment_id': comment.id,\n",
    "                                    'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                                    'body': comment.body,\n",
    "                                    'score': comment.score,\n",
    "                                    'timestamp': datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                                    'parent_id': comment.parent_id\n",
    "                                }\n",
    "                                comments_data.append(comment_data)\n",
    "                            \n",
    "                            post_data = {\n",
    "                                'post_id': post.id,\n",
    "                                'timestamp': datetime.fromtimestamp(post.created_utc).isoformat(),\n",
    "                                'title': post.title,\n",
    "                                'content': post.selftext,\n",
    "                                'subreddit': subreddit_name,\n",
    "                                'ups': post.ups,\n",
    "                                'num_comments': post.num_comments,\n",
    "                                'url': post.url,\n",
    "                                'comments': comments_data\n",
    "                            }\n",
    "                            \n",
    "                            extracted_posts.append(post_data)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subreddit {subreddit_name}: {e}\")\n",
    "        \n",
    "        return extracted_posts\n",
    "    \n",
    "    def _contains_keywords(self, text):\n",
    "        \"\"\"\n",
    "        Check if text contains any of the predefined keywords\n",
    "        \"\"\"\n",
    "        return any(keyword.lower() in text for keyword in self.keywords)\n",
    "\n",
    "    def save_to_csv(self, posts, filename='mental_health_posts.csv'):\n",
    "        \"\"\"\n",
    "        Save extracted posts to CSV\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(posts)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(posts)} posts to {filename}\")\n",
    "\n",
    "    def save_to_json(self, posts, filename='mental_health_posts.json'):\n",
    "        \"\"\"\n",
    "        Save extracted posts to JSON\n",
    "        \"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(posts)} posts to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Reddit API credentials\n",
    "    CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "    CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "    USER_AGENT = 'SocialAnalysis<v0>(by /u/Sad-Net-4568)'\n",
    "    \n",
    "    # Target subreddit partial names to search\n",
    "    target_subreddits = [\n",
    "        'mentalhealth', 'depression', 'anxiety', \n",
    "        'addiction', 'support', 'mental', 'psychiatry', 'ketamine', 'SuicideWatch',\n",
    "        'bipolar', 'ptsd', 'trauma', 'burnout', 'emotional', 'intrusivethoughts', 'stopdrinking',\n",
    "        'leaves', 'stopsmoking', 'stopdrugs', 'stopselfharm', 'opiatesrecovery', 'recovery', 'therapy', 'crisis',\n",
    "        'selfharm', 'stress', 'panic'\n",
    "    ]\n",
    "\n",
    "    scraper = RedditMentalHealthScraper(CLIENT_ID, CLIENT_SECRET, USER_AGENT)\n",
    "    \n",
    "    # Used in analysis\n",
    "    extracted_posts = scraper.extract_posts(target_subreddits, limit=1000)\n",
    "\n",
    "    # Tested with comment extraction but not used in further analysis\n",
    "    # extracted_posts = scraper.extract_posts_with_comments(target_subreddits, limit=100, comment_limit=50, replace_more_limit=1)\n",
    "    \n",
    "    scraper.save_to_csv(posts=extracted_posts, filename='mental_health_postsV1.csv')\n",
    "    # scraper.save_to_json(posts=extracted_posts, filename='mental_health_postsV1.json') # Not used as CSV is easier to work with in Pandas\n",
    "    print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching subreddits found: ['mentalhealth', 'MentalHealthPH', 'MentalHealthSupport', 'MentalHealthUK', 'MentalHealthIsland', 'MentalHealthBabies', 'MentalHealthProviders', 'MentalHealthPros', 'mentalhealthadvice']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [09:37<00:00, 64.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1018 posts to mental_health_postswith_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would analyze comments in the Project,\n",
    "\n",
    "I am positive about how it can gives us more data around more users, not only the person who posted the post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "It's been done for data that doesn't have comments-content\n",
    "\n",
    "Reason- Avoided for now, because of it's high api hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/nlp_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/nlp_preprocessing.py\n",
    "\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer # didn't used\n",
    "import re\n",
    "import emoji\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# this is file that I saved, provided in repo also\n",
    "df = pd.read_csv('mental_health_postsV1.csv') # use the file saved via reddit extraction code\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - {'no', 'not', 'nor', 'never'} # don't remove negation words\n",
    "\n",
    "def preprocess_text_with_counts(text):\n",
    "    if pd.isnull(text):\n",
    "        return text, 0, 0, 0\n",
    "\n",
    "    emoji_count = len([char for char in text if char in emoji.EMOJI_DATA])\n",
    "    special_char_count = len(re.findall(r'[^A-Za-z\\s]', text))\n",
    "    stopword_count = len([word for word in text.split() if word.lower() in stop_words])\n",
    "\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text, emoji_count, special_char_count, stopword_count\n",
    "\n",
    "original_content = data['content'].copy()\n",
    "emoji_changes = []\n",
    "special_char_changes = []\n",
    "stopword_changes = []\n",
    "\n",
    "def process_row(row):\n",
    "    processed_text, emoji_count, special_char_count, stopword_count = preprocess_text_with_counts(row)\n",
    "    emoji_changes.append(emoji_count)\n",
    "    special_char_changes.append(special_char_count)\n",
    "    stopword_changes.append(stopword_count)\n",
    "    return processed_text\n",
    "\n",
    "for index, row in tqdm(data['content'].items(), total=len(data['content']), desc=\"Processing rows\"):\n",
    "    data.at[index, 'content'] = process_row(row)\n",
    "\n",
    "changed_rows = (original_content != data['content']).sum()\n",
    "data.to_csv('mental_health_postsV1_preprocessed.csv', index=False)\n",
    "\n",
    "# Calculate total counts and rows with changes\n",
    "total_emoji_removed = sum(emoji_changes)\n",
    "total_special_chars_removed = sum(special_char_changes)\n",
    "total_stopwords_removed = sum(stopword_changes)\n",
    "\n",
    "rows_with_emoji_changes = sum(1 for count in emoji_changes if count > 0)\n",
    "rows_with_special_char_changes = sum(1 for count in special_char_changes if count > 0)\n",
    "rows_with_stopword_changes = sum(1 for count in stopword_changes if count > 0)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows changed: {changed_rows}\")\n",
    "print(f\"Total emoji removed: {total_emoji_removed}\")\n",
    "print(f\"Rows with emoji changes: {rows_with_emoji_changes}\")\n",
    "print(f\"Total special characters removed: {total_special_chars_removed}\")\n",
    "print(f\"Rows with special character changes: {rows_with_special_char_changes}\")\n",
    "print(f\"Total stopwords removed: {total_stopwords_removed}\")\n",
    "print(f\"Rows with stopword changes: {rows_with_stopword_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1gd9l9c</td>\n",
       "      <td>2024-10-27T17:43:33</td>\n",
       "      <td>Elections and Politics</td>\n",
       "      <td>hello friends time year always intended rmenta...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1e297nd</td>\n",
       "      <td>2024-07-13T17:55:58</td>\n",
       "      <td>r/MentalHealth is looking for moderators</td>\n",
       "      <td>hey rmentalhealth looking grow moderation team...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1jl6wjb</td>\n",
       "      <td>2025-03-27T21:11:46</td>\n",
       "      <td>Being weak physically as a man makes me depres...</td>\n",
       "      <td>yo cant even lateral raises kg dumbbell basica...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1jlb42o</td>\n",
       "      <td>2025-03-28T00:05:58</td>\n",
       "      <td>Do you think you are losing your youth because...</td>\n",
       "      <td>mid twenties mental health declining much affe...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1jl422s</td>\n",
       "      <td>2025-03-27T19:05:37</td>\n",
       "      <td>Medication withdrawl (Effexor, Venlafaxine)</td>\n",
       "      <td>call help hello im f french ive taking venlafa...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>https://i.redd.it/gnn4t73bj8re1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1jl5xhf</td>\n",
       "      <td>2025-03-27T20:30:15</td>\n",
       "      <td>Burnt Out On Self Care</td>\n",
       "      <td>diagnosed cptsd gad mood disorder depressive f...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>https://i.redd.it/b7zgxgrey8re1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1jkxutf</td>\n",
       "      <td>2025-03-27T12:14:06</td>\n",
       "      <td>Shoutout Mental Health Muscle, I am proud to b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>https://i.redd.it/nhadpq2wh6re1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1jl5tea</td>\n",
       "      <td>2025-03-27T20:25:32</td>\n",
       "      <td>Feeling hopeless</td>\n",
       "      <td>im feeling completely hopeless made many wrong...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1jkwbcw</td>\n",
       "      <td>2025-03-27T10:26:07</td>\n",
       "      <td>What’s One Small Habit That Has Helped Your Me...</td>\n",
       "      <td>sometimes small changes make big difference ma...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1jl97wc</td>\n",
       "      <td>2025-03-27T22:48:03</td>\n",
       "      <td>I just want to leave my phone and runaway. I f...</td>\n",
       "      <td>feel lost alone overwhelmed feel like ive abso...</td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/mentalhealth/comments...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id            timestamp  \\\n",
       "0  1gd9l9c  2024-10-27T17:43:33   \n",
       "1  1e297nd  2024-07-13T17:55:58   \n",
       "2  1jl6wjb  2025-03-27T21:11:46   \n",
       "3  1jlb42o  2025-03-28T00:05:58   \n",
       "4  1jl422s  2025-03-27T19:05:37   \n",
       "5  1jl5xhf  2025-03-27T20:30:15   \n",
       "6  1jkxutf  2025-03-27T12:14:06   \n",
       "7  1jl5tea  2025-03-27T20:25:32   \n",
       "8  1jkwbcw  2025-03-27T10:26:07   \n",
       "9  1jl97wc  2025-03-27T22:48:03   \n",
       "\n",
       "                                               title  \\\n",
       "0                             Elections and Politics   \n",
       "1           r/MentalHealth is looking for moderators   \n",
       "2  Being weak physically as a man makes me depres...   \n",
       "3  Do you think you are losing your youth because...   \n",
       "4        Medication withdrawl (Effexor, Venlafaxine)   \n",
       "5                             Burnt Out On Self Care   \n",
       "6  Shoutout Mental Health Muscle, I am proud to b...   \n",
       "7                                   Feeling hopeless   \n",
       "8  What’s One Small Habit That Has Helped Your Me...   \n",
       "9  I just want to leave my phone and runaway. I f...   \n",
       "\n",
       "                                             content     subreddit  ups  \\\n",
       "0  hello friends time year always intended rmenta...  mentalhealth   24   \n",
       "1  hey rmentalhealth looking grow moderation team...  mentalhealth   21   \n",
       "2  yo cant even lateral raises kg dumbbell basica...  mentalhealth   22   \n",
       "3  mid twenties mental health declining much affe...  mentalhealth    9   \n",
       "4  call help hello im f french ive taking venlafa...  mentalhealth   20   \n",
       "5  diagnosed cptsd gad mood disorder depressive f...  mentalhealth   14   \n",
       "6                                                NaN  mentalhealth   41   \n",
       "7  im feeling completely hopeless made many wrong...  mentalhealth    5   \n",
       "8  sometimes small changes make big difference ma...  mentalhealth   30   \n",
       "9  feel lost alone overwhelmed feel like ive abso...  mentalhealth    3   \n",
       "\n",
       "   num_comments                                                url  \n",
       "0            17  https://www.reddit.com/r/mentalhealth/comments...  \n",
       "1            27  https://www.reddit.com/r/mentalhealth/comments...  \n",
       "2            23  https://www.reddit.com/r/mentalhealth/comments...  \n",
       "3             9  https://www.reddit.com/r/mentalhealth/comments...  \n",
       "4             9               https://i.redd.it/gnn4t73bj8re1.jpeg  \n",
       "5             2               https://i.redd.it/b7zgxgrey8re1.jpeg  \n",
       "6             2               https://i.redd.it/nhadpq2wh6re1.jpeg  \n",
       "7             4  https://www.reddit.com/r/mentalhealth/comments...  \n",
       "8            41  https://www.reddit.com/r/mentalhealth/comments...  \n",
       "9             1  https://www.reddit.com/r/mentalhealth/comments...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mental_health_postsV1_preprocessed.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to consider\n",
    "As said in Task, i have removed special-characters and emojis.\n",
    "\n",
    "Although they may help in getting better context of post-how much difference it will create. that needs to be look upon.\n",
    "\n",
    "I think it won't be much, main challenge can be understanding of emoticons with text, **with understanding of sarcasm**\n",
    "\n",
    "**Comments content not used in this analysis, But in main Project I will definitely use it - script for it is provided too.**\n",
    "- Work Upon creating a lexicon for crisis-terms, if it works sufficiently in comparison to analyze whole text via ML-methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Classification using bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/risk_classification_bart.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def classify_post_risk(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify social media posts into risk levels using zero-shot classification.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with 'post_id' and 'content' columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with added 'risk_level' column\n",
    "    \"\"\"\n",
    "    # Initialize zero-shot classification pipeline\n",
    "    # Using a robust model for nuanced classification\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\", \n",
    "        model=\"facebook/bart-large-mnli\"\n",
    "    )\n",
    "    \n",
    "    # Define risk level categories with descriptive labels\n",
    "    risk_categories = [\n",
    "        \"High-Risk: Immediate Crisis\",\n",
    "        \"Moderate Concern: Seeking Help\",\n",
    "        \"Low Concern: General Discussion\"\n",
    "    ]\n",
    "    \n",
    "    # Classification function with detailed criteria\n",
    "    def determine_risk_level(text):\n",
    "        try:\n",
    "            # Perform zero-shot classification\n",
    "            result = classifier(\n",
    "                text, \n",
    "                candidate_labels=risk_categories, \n",
    "                hypothesis_template=\"This text indicates {}\"\n",
    "            )\n",
    "            \n",
    "            # Get the top predicted category\n",
    "            top_category = result['labels'][0]\n",
    "            \n",
    "            if \"High-Risk\" in top_category:\n",
    "                return \"High-Risk\"\n",
    "            elif \"Moderate Concern\" in top_category:\n",
    "                return \"Moderate Concern\"\n",
    "            else:\n",
    "                return \"Low Concern\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying text: {text}\")\n",
    "            return \"Unclassified\"\n",
    "    \n",
    "    # Apply risk classification to the DataFrame\n",
    "    tqdm.pandas(desc=\"Classifying posts\")\n",
    "    df['risk_level'] = df['content'].progress_apply(determine_risk_level)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "def main(df:pd.DataFrame): \n",
    "    # Classify posts\n",
    "    classified_df = classify_post_risk(df)\n",
    "    # classified_df.to_csv('mental_health_postsV1_classified.csv', index=False)\n",
    "    # Display results\n",
    "    print(classified_df)\n",
    "\n",
    "# Additional helper functions for advanced analysis\n",
    "def get_risk_level_summary(df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics of risk levels\n",
    "    \"\"\"\n",
    "    risk_summary = df['risk_level'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nRisk Level Distribution:\")\n",
    "    print(risk_summary)\n",
    "    return risk_summary\n",
    "\n",
    "def identify_high_risk_posts(df):\n",
    "    \"\"\"\n",
    "    Extract and highlight high-risk posts\n",
    "    \"\"\"\n",
    "    high_risk_posts = df[df['risk_level'] == 'High-Risk']\n",
    "    print(\"\\nHigh-Risk Posts:\")\n",
    "    print(high_risk_posts)\n",
    "    return high_risk_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a time consuming process, better to evaluate it from provided saved file\n",
    "\n",
    "# Uncomment to run\n",
    "# if __name__ == \"__main__\":\n",
    "#     data = pd.read_csv('mental_health_postsV1_preprocessed.csv')\n",
    "#     df = classify_post_risk(data)\n",
    "#     df.to_csv('mental_health_postsV1_classified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mental_health_postsV1_classified.csv') # change with the name file you saved\n",
    "get_risk_level_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is distributed for parallel\n",
    "\n",
    "But better to not run locally as I ran on kaagle's T4x2 GPU, so this can consume more time locally.\n",
    "\n",
    "Plots are available via plotly -> .html files\n",
    "\n",
    "You can just refer **dashboard2.png** for plots of this section\n",
    "\n",
    "used model: all-MiniLM-L6-v2\n",
    "\n",
    "**Things to try:**\n",
    "A higher-dimension model like: sentence-transformers/gtr-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/sentiment_classification.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from functools import partial\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Parallel VADER sentiment analysis\n",
    "def process_chunk(chunk_df, text_column='content'):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    chunk_df['sentiment_scores'] = chunk_df[text_column].apply(\n",
    "        lambda text: sid.polarity_scores(str(text)) if pd.notna(text) else {'compound': 0}\n",
    "    )\n",
    "    chunk_df['sentiment_score'] = chunk_df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "    \n",
    "    def categorize_sentiment(score):\n",
    "        if score >= 0.05: return 'Positive'\n",
    "        elif score <= -0.05: return 'Negative'\n",
    "        else: return 'Neutral'\n",
    "    \n",
    "    chunk_df['sentiment'] = chunk_df['sentiment_score'].apply(categorize_sentiment)\n",
    "    chunk_df = chunk_df.drop('sentiment_scores', axis=1)\n",
    "    return chunk_df\n",
    "\n",
    "def analyze_sentiment_vader_parallel(df, text_column='content', batch_size=1000, n_jobs=None):\n",
    "    \"\"\"\n",
    "    Parallel implementation of VADER sentiment analysis\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Split dataframe into chunks for parallel processing\n",
    "    df_chunks = np.array_split(result_df, n_jobs)\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        processed_chunks = list(tqdm(\n",
    "            executor.map(partial(process_chunk, text_column=text_column), df_chunks),\n",
    "            total=len(df_chunks),\n",
    "            desc=\"Sentiment Analysis\"\n",
    "        ))\n",
    "    \n",
    "    # Combine results\n",
    "    result_df = pd.concat(processed_chunks)\n",
    "    return result_df\n",
    "    \n",
    "# Custom PyTorch Dataset for BERT embeddings\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# Multi-GPU BERT embeddings function\n",
    "def get_bert_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=32, num_gpus=None):\n",
    "    \"\"\"\n",
    "    Generate embeddings using BERT with multi-GPU support\n",
    "    \"\"\"\n",
    "    if num_gpus is None:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    if num_gpus == 0:\n",
    "        # Fall back to CPU if no GPUs available\n",
    "        model = SentenceTransformer(model_name)\n",
    "        return model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    \n",
    "    # Create multiple model instances for different GPUs\n",
    "    models = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        device = f\"cuda:{gpu_id}\"\n",
    "        model = SentenceTransformer(model_name)\n",
    "        model.to(device)\n",
    "        models.append((model, device))\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TextDataset(texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size * num_gpus, shuffle=False)\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Generating BERT embeddings\"):\n",
    "        # Split batch for each GPU\n",
    "        batch_size_per_gpu = len(batch) // num_gpus\n",
    "        batch_splits = []\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            start_idx = i * batch_size_per_gpu\n",
    "            end_idx = start_idx + batch_size_per_gpu if i < num_gpus - 1 else len(batch)\n",
    "            batch_splits.append(batch[start_idx:end_idx])\n",
    "        \n",
    "        # Process in parallel across GPUs\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        def encode_on_gpu(model_device, batch_text):\n",
    "            model, device = model_device\n",
    "            return model.encode(batch_text)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_gpus) as executor:\n",
    "            batch_results = list(executor.map(\n",
    "                encode_on_gpu, \n",
    "                models,\n",
    "                batch_splits\n",
    "            ))\n",
    "        \n",
    "        # Combine results\n",
    "        for result in batch_results:\n",
    "            batch_embeddings.append(result)\n",
    "            \n",
    "        combined_embeddings = np.vstack(batch_embeddings)\n",
    "        all_embeddings.append(combined_embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return []\n",
    "    return word_tokenize(text.lower())\n",
    "    \n",
    "def calculate_crisis_score(tokens, crisis_terms_set):\n",
    "    if not tokens:\n",
    "        return 0, []\n",
    "    \n",
    "    # Find crisis terms in post\n",
    "    found_terms = [t for t in tokens if t in crisis_terms_set]\n",
    "    \n",
    "    # Calculate score based on number of crisis terms\n",
    "    score = len(found_terms) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return score, found_terms\n",
    "\n",
    "def extract_terms_from_text(text_series, min_words=2, workers=None):\n",
    "    if workers is None:\n",
    "        workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        all_words_lists = list(executor.map(tokenize_text, text_series))\n",
    "    \n",
    "    # Flatten list of lists\n",
    "    all_words = [word for sublist in all_words_lists for word in sublist]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = pd.Series(all_words).value_counts()\n",
    "    \n",
    "    # Return top words\n",
    "    return word_counts.head(20).index.tolist()\n",
    "    \n",
    "# Tokenize in parallel\n",
    "def tokenize_text(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return []\n",
    "    words = word_tokenize(text.lower())\n",
    "    min_words = 2\n",
    "    return [w for w in words if len(w) > min_words and w.isalpha()]\n",
    "    \n",
    "\n",
    "def find_crisis_terms(text, terms):\n",
    "    if pd.isna(text) or text == '' or not terms:\n",
    "        return []\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    found_terms = [term for term in terms if term in text_lower]\n",
    "    return found_terms\n",
    "\n",
    "\n",
    "# Detect high-risk terms using parallel BERT\n",
    "def detect_crisis_terms_bert_parallel(df, text_column='content', title_column='title', batch_size=32, num_gpus=None):\n",
    "    \"\"\"\n",
    "    Detect high-risk crisis terms using BERT with multi-GPU support\n",
    "    \"\"\"\n",
    "    if num_gpus is None:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Combine title and content\n",
    "    result_df['combined_text'] = result_df.apply(\n",
    "        lambda row: str(row[title_column] or '') + ' ' + str(row[text_column] or ''), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Get post embeddings using multi-GPU\n",
    "    posts = result_df['combined_text'].fillna('').tolist()\n",
    "    if not posts:\n",
    "        print(\"Warning: No valid text found for BERT model\")\n",
    "        result_df['crisis_score'] = 0\n",
    "        result_df['high_risk_terms'] = [[] for _ in range(len(result_df))]\n",
    "        return result_df\n",
    "    \n",
    "    post_embeddings = get_bert_embeddings(\n",
    "        texts=posts,\n",
    "        batch_size=batch_size,\n",
    "        num_gpus=num_gpus\n",
    "    )\n",
    "    \n",
    "    print(\"Clustering posts to identify high-risk content...\")\n",
    "    \n",
    "    # Use KMeans to cluster posts\n",
    "    n_clusters = min(5, len(posts))  # Use fewer clusters for smaller datasets\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(post_embeddings)\n",
    "    \n",
    "    # Identify high-risk cluster by looking for posts labeled as high-risk\n",
    "    high_risk_posts_idx = result_df['risk_level'] == 'High-Risk'\n",
    "    \n",
    "    if high_risk_posts_idx.any():\n",
    "        # Count posts in each cluster by risk level\n",
    "        cluster_risk_counts = pd.crosstab(\n",
    "            index=cluster_labels, \n",
    "            columns=result_df['risk_level']\n",
    "        )\n",
    "        \n",
    "        # Identify clusters with higher proportion of high-risk posts\n",
    "        if 'High-Risk' in cluster_risk_counts.columns:\n",
    "            # Calculate proportion of high-risk posts in each cluster\n",
    "            cluster_risk_props = cluster_risk_counts.div(\n",
    "                cluster_risk_counts.sum(axis=1), axis=0\n",
    "            )\n",
    "            \n",
    "            # Get clusters with high proportion of high-risk posts\n",
    "            high_risk_clusters = cluster_risk_props[\n",
    "                cluster_risk_props['High-Risk'] > 0.3\n",
    "            ].index.tolist()\n",
    "        else:\n",
    "            high_risk_clusters = []\n",
    "    else:\n",
    "        # If no known high-risk posts, look for outlier clusters (smallest clusters)\n",
    "        cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "        high_risk_clusters = cluster_counts[cluster_counts < cluster_counts.median()].index.tolist()\n",
    "    \n",
    "    # Set crisis score based on cluster membership\n",
    "    result_df['crisis_score'] = [\n",
    "        0.9 if label in high_risk_clusters else 0.1 for label in cluster_labels\n",
    "    ]\n",
    "    \n",
    "    # Extract keywords from high-risk clusters in parallel\n",
    "    print(\"Extracting crisis terms from high-risk clusters...\")\n",
    "    \n",
    "    # Function to extract most common words\n",
    "    \n",
    "    # Extract terms from high-risk clusters\n",
    "    if high_risk_clusters:\n",
    "        high_risk_mask = [label in high_risk_clusters for label in cluster_labels]\n",
    "        high_risk_texts = result_df.loc[high_risk_mask, 'combined_text']\n",
    "        \n",
    "        workers = multiprocessing.cpu_count()\n",
    "        crisis_terms = extract_terms_from_text(high_risk_texts, workers=workers)\n",
    "        print(f\"Extracted {len(crisis_terms)} crisis terms\")\n",
    "    else:\n",
    "        crisis_terms = []\n",
    "\n",
    "    # If crisis_terms is defined\n",
    "    if not crisis_terms:\n",
    "        print(\"Warning: No crisis terms found. Using fallback terms.\")\n",
    "        crisis_terms = [\n",
    "            'suicide', 'kill myself', 'end my life', 'die', 'better off dead', \n",
    "            'no reason to live', 'can\\'t go on', 'take my own life', 'ending it all',\n",
    "            'want to die', 'don\\'t want to be here', 'give up', 'hopeless',\n",
    "            'self harm', 'cut myself', 'hurt myself', 'overdose', 'pills',\n",
    "            'worthless', 'burden', 'no purpose', 'no point', 'saying goodbye',\n",
    "            'last post', 'final note', 'throwaway account', 'final post', \n",
    "            'delete this later', 'not going to respond', 'just needed to say this'\n",
    "        ]\n",
    "        result_df = result_df.drop(['crisis_score'], axis=1)\n",
    "        \n",
    "    # Saving crisis_terms\n",
    "    with open('crisis_terms.txt', 'w') as f:\n",
    "        f.write(str(crisis_terms))\n",
    "    # Apply to each post in parallel\n",
    "    find_terms = partial(find_crisis_terms, terms=crisis_terms)\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "        high_risk_terms = list(tqdm(\n",
    "            executor.map(find_terms, result_df['combined_text']),\n",
    "            total=len(result_df),\n",
    "            desc=\"Finding crisis terms in posts\"\n",
    "        ))\n",
    "    \n",
    "    result_df['high_risk_terms'] = high_risk_terms\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    result_df = result_df.drop('combined_text', axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Interactive HTML Visualizations with Plotly\n",
    "def create_interactive_visualizations(df, output_dir='.', method:str='bert'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_counts = df['sentiment'].value_counts().reset_index()\n",
    "    sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "    \n",
    "    fig1 = px.bar(\n",
    "        sentiment_counts, \n",
    "        x='Sentiment', \n",
    "        y='Count',\n",
    "        color='Sentiment',\n",
    "        title='Distribution of Posts by Sentiment',\n",
    "        color_discrete_map={'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'}\n",
    "    )\n",
    "    \n",
    "    fig1.update_layout(\n",
    "        xaxis_title='Sentiment',\n",
    "        yaxis_title='Number of Posts',\n",
    "        legend_title='Sentiment'\n",
    "    )\n",
    "    \n",
    "    fig1.write_html(f\"{output_dir}/sentiment_{method}distribution.html\")\n",
    "    \n",
    "    # Sentiment by risk level\n",
    "    fig2 = px.histogram(\n",
    "        df, \n",
    "        x='risk_level',\n",
    "        color='sentiment',\n",
    "        barmode='group',\n",
    "        title='Distribution of Posts by Risk Level and Sentiment',\n",
    "        color_discrete_map={'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'}\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        xaxis_title='Risk Level',\n",
    "        yaxis_title='Number of Posts',\n",
    "        legend_title='Sentiment'\n",
    "    )\n",
    "    \n",
    "    fig2.write_html(f\"{output_dir}/sentiment_{method}_by_risk.html\")\n",
    "\n",
    "    if 'crisis_score' in df.columns:\n",
    "        # Crisis score by risk level\n",
    "        fig3 = px.box(\n",
    "            df,\n",
    "            x='risk_level',\n",
    "            y='crisis_score',\n",
    "            color='risk_level',\n",
    "            title='Crisis Scores by Risk Level'\n",
    "        )\n",
    "        \n",
    "        fig3.update_layout(\n",
    "            xaxis_title='Risk Level',\n",
    "            yaxis_title='Crisis Score'\n",
    "        )\n",
    "        \n",
    "        fig3.write_html(f\"{output_dir}/crisis_score_{method}_by_risk.html\")\n",
    "    \n",
    "    # Heatmap of sentiment vs risk level\n",
    "    crosstab = pd.crosstab(df['risk_level'], df['sentiment'])\n",
    "    \n",
    "    fig4 = px.imshow(\n",
    "        crosstab,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title='Heatmap of Risk Level vs Sentiment',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig4.update_layout(\n",
    "        xaxis_title='Sentiment',\n",
    "        yaxis_title='Risk Level'\n",
    "    )\n",
    "    \n",
    "    fig4.write_html(f\"{output_dir}/sentiment_risk_{method}_heatmap.html\")\n",
    "    \n",
    "    # Top high-risk terms\n",
    "    all_terms = []\n",
    "    for terms in df['high_risk_terms']:\n",
    "        all_terms.extend(terms)\n",
    "    \n",
    "    if all_terms:\n",
    "        term_counts = pd.Series(all_terms).value_counts().reset_index()\n",
    "        term_counts.columns = ['Term', 'Frequency']\n",
    "        term_counts = term_counts.sort_values('Frequency', ascending=False).head(15)\n",
    "        \n",
    "        fig5 = px.bar(\n",
    "            term_counts,\n",
    "            x='Term',\n",
    "            y='Frequency',\n",
    "            title='Top 15 High-Risk Terms',\n",
    "            color='Frequency',\n",
    "            color_continuous_scale='Reds'\n",
    "        )\n",
    "        \n",
    "        fig5.update_layout(\n",
    "            xaxis_title='Term',\n",
    "            yaxis_title='Frequency',\n",
    "            xaxis={'categoryorder':'total descending'}\n",
    "        )\n",
    "    else:\n",
    "        # Create empty figure if no terms\n",
    "        fig5 = go.Figure()\n",
    "        fig5.update_layout(\n",
    "            title=\"No High-Risk Terms Detected\",\n",
    "            xaxis_title=\"Term\",\n",
    "            yaxis_title=\"Frequency\"\n",
    "        )\n",
    "    \n",
    "    fig5.write_html(f\"{output_dir}/top_crisis_{method}_terms.html\")\n",
    "    \n",
    "    # Create dashboard\n",
    "    from plotly.subplots import make_subplots\n",
    "    if 'crisis_score'in df.columns:\n",
    "        fig = make_subplots(\n",
    "            rows=2, \n",
    "            cols=3,\n",
    "            subplot_titles=(\n",
    "                \"Sentiment Distribution\",\n",
    "                \"Risk Level vs Sentiment\",\n",
    "                \"Crisis Scores by Risk Level\",\n",
    "                \"Risk Level vs Sentiment Heatmap\",\n",
    "                \"Top High-Risk Terms\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add traces to subplots\n",
    "        for trace in fig1.data:\n",
    "            fig.add_trace(trace, row=1, col=1)\n",
    "        \n",
    "        for trace in fig2.data:\n",
    "            fig.add_trace(trace, row=1, col=2)\n",
    "    \n",
    "        for trace in fig3.data:\n",
    "            fig.add_trace(trace, row=1, col=3)\n",
    "            \n",
    "        for trace in fig4.data:\n",
    "            fig.add_trace(trace, row=2, col=1)\n",
    "            \n",
    "        for trace in fig5.data:\n",
    "            fig.add_trace(trace, row=2, col=2)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title_text=\"Reddit Post Analysis Dashboard\",\n",
    "            height=900,\n",
    "            width=1500,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.write_html(f\"{output_dir}/dashboard_{method}_.html\")\n",
    "        \n",
    "        return fig1, fig2, fig3, fig4, fig5\n",
    "\n",
    "    else:\n",
    "        fig = make_subplots(\n",
    "            rows=2, \n",
    "            cols=2,\n",
    "            subplot_titles=(\n",
    "                \"Sentiment Distribution\",\n",
    "                \"Risk Level vs Sentiment\",\n",
    "                \"Risk Level vs Sentiment Heatmap\",\n",
    "                \"Top High-Risk Terms\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add traces to subplots\n",
    "        for trace in fig1.data:\n",
    "            fig.add_trace(trace, row=1, col=1)\n",
    "        \n",
    "        for trace in fig2.data:\n",
    "            fig.add_trace(trace, row=1, col=2)\n",
    "            \n",
    "        for trace in fig4.data:\n",
    "            fig.add_trace(trace, row=2, col=1)\n",
    "            \n",
    "        for trace in fig5.data:\n",
    "            fig.add_trace(trace, row=2, col=2)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title_text=\"Reddit Post Analysis Dashboard\",\n",
    "            height=900,\n",
    "            width=1500,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.write_html(f\"{output_dir}/dashboard_{method}_.html\")\n",
    "        return fig1, fig2, fig4, fig5\n",
    "\n",
    "# Main execution function\n",
    "def process_reddit_data(df, output_dir='.', method='bert', batch_size=32, num_gpus=None):\n",
    "    \"\"\"\n",
    "    Process Reddit data with parallel sentiment analysis and crisis term detection.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing Reddit posts\n",
    "        output_dir: Directory to save HTML plots\n",
    "        method: Detection method - 'bert' or 'word2vec'\n",
    "        batch_size: Batch size for processing\n",
    "        num_gpus: Number of GPUs to use (None = auto-detect)\n",
    "    \n",
    "    Returns:\n",
    "        Processed DataFrame with sentiment and crisis term data\n",
    "    \"\"\"\n",
    "    # Auto-detect GPU count if not specified\n",
    "    if num_gpus is None:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Using {num_gpus} GPUs for processing\")\n",
    "    \n",
    "    workers = multiprocessing.cpu_count()\n",
    "    print(f\"Using {workers} CPU cores for parallel processing\")\n",
    "    \n",
    "    # Step 1: Parallel Sentiment Analysis\n",
    "    print(\"Performing parallel sentiment analysis...\")\n",
    "    df_with_sentiment = analyze_sentiment_vader_parallel(df, n_jobs=workers)\n",
    "    \n",
    "    print(\"Detecting high-risk crisis terms with multi-GPU BERT...\")\n",
    "    final_df = detect_crisis_terms_bert_parallel(\n",
    "        df_with_sentiment, \n",
    "        batch_size=batch_size,\n",
    "        num_gpus=num_gpus\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create and save interactive visualizations\n",
    "    print(\"Creating interactive visualizations...\")\n",
    "    create_interactive_visualizations(final_df, output_dir, method)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(final_df['sentiment'].value_counts())\n",
    "    \n",
    "    print(\"\\nSentiment by Risk Level:\")\n",
    "    print(pd.crosstab(final_df['risk_level'], final_df['sentiment']))\n",
    "    \n",
    "\n",
    "    all_terms = []\n",
    "    for terms in final_df['high_risk_terms']:\n",
    "        all_terms.extend(terms)\n",
    "    \n",
    "    if all_terms:\n",
    "        print(\"\\nTop 20 Detected Crisis Terms:\")\n",
    "        print(pd.Series(all_terms).value_counts().head(20))\n",
    "    \n",
    "    # Sample high-risk posts with negative sentiment\n",
    "    high_risk_negative = final_df[\n",
    "        (final_df['risk_level'] == 'High-Risk') & \n",
    "        (final_df['sentiment'] == 'Negative')\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTop 5 High-Risk Negative Posts (by crisis score):\")\n",
    "    if len(high_risk_negative) > 0:\n",
    "        for idx, row in high_risk_negative.head().iterrows():\n",
    "            print(f\"Post ID: {row['post_id']}\")\n",
    "            print(f\"Title: {row['title']}\")\n",
    "            print(f\"Risk Terms: {row['high_risk_terms']}\")\n",
    "            print(f\"Sentiment Score: {row['sentiment_score']:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can just refer dashboard_bert.html for plots'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('mental_health_postsV1_classified.csv')\n",
    "\n",
    "# this line ran the above process, uncomment to run it\n",
    "# processed_df = process_reddit_data(df, method='bert', output_dir='results_bert')\n",
    "# processed_df.to_csv('crisis_terms_processed.csv')\n",
    "\n",
    "\"\"\"You can just refer plots\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Moderate Concern takes the majority of posts > Low Concern> High Concern\n",
    "- Negative Posts > Positive Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Geographical data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method - 1\n",
    "\n",
    "using standard nlp techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/geographical_extract_nlp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/geographical_extract_nlp.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy model for NLP-based place recognition\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_locations(text):\n",
    "    \"\"\"Extract location entities from text using spaCy\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    locations = []\n",
    "    \n",
    "    # Extract GPE (Geo-Political Entity) and LOC (Location) entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            locations.append(ent.text)\n",
    "    \n",
    "    return locations\n",
    "\n",
    "def geocode_location(location_name, geolocator):\n",
    "    \"\"\"Convert location name to coordinates using geocoding\"\"\"\n",
    "    try:\n",
    "        # Add 'USA' to improve geocoding accuracy for US locations\n",
    "        location = geolocator.geocode(location_name, exactly_one=True) # removed US bias, let's see results\n",
    "        if location is None:\n",
    "            location = geolocator.geocode(location_name, exactly_one=True)\n",
    "        \n",
    "        if location:\n",
    "            return {\n",
    "                'location': location_name,\n",
    "                'lat': location.latitude,\n",
    "                'lon': location.longitude,\n",
    "                'address': location.address\n",
    "            }\n",
    "        return None\n",
    "    except (GeocoderTimedOut, GeocoderUnavailable):\n",
    "        # Handle timeout errors by waiting and retrying once\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            location = geolocator.geocode(location_name, exactly_one=True)\n",
    "            if location:\n",
    "                return {\n",
    "                    'location': location_name,\n",
    "                    'lat': location.latitude,\n",
    "                    'lon': location.longitude,\n",
    "                    'address': location.address\n",
    "                }\n",
    "            return None\n",
    "        except (GeocoderTimedOut, GeocoderUnavailable):\n",
    "            return None\n",
    "\n",
    "def process_dataset(df:pd.DataFrame):\n",
    "    \"\"\"Process the dataset to extract and geocode locations\"\"\"\n",
    "    print(\"Extracting locations from posts...\")\n",
    "    \n",
    "    # Initialize a geolocator with a custom user agent\n",
    "    geolocator = Nominatim(user_agent=\"crisis_mapping_tool\")\n",
    "    \n",
    "    # Combine title and content for better location extraction\n",
    "    df['full_text'] = df['title'].fillna('') + ' ' + df['content'].fillna('')\n",
    "    \n",
    "    # Extract locations from text\n",
    "    df['extracted_locations'] = df['full_text'].apply(extract_locations)\n",
    "    \n",
    "    df.to_csv('mental_health_postsV1_extracted_unbiased_locations.csv', index=False)\n",
    "    # Flatten the list of locations and count occurrences\n",
    "    all_locations = []\n",
    "    for locations in df['extracted_locations']:\n",
    "        all_locations.extend(locations)\n",
    "    \n",
    "    location_counts = Counter(all_locations)\n",
    "    \n",
    "    # Get the top 50 locations for geocoding (to avoid excessive API calls)\n",
    "    top_locations = location_counts.most_common(50)\n",
    "    \n",
    "    print(f\"Top 10 most mentioned locations: {top_locations[:10]}\")\n",
    "    \n",
    "    # Geocode the top locations\n",
    "    geocoded_locations = {}\n",
    "    print(\"Geocoding top locations...\")\n",
    "    for location_name, count in tqdm(top_locations):\n",
    "        if location_name not in geocoded_locations:\n",
    "            geo_info = geocode_location(location_name, geolocator)\n",
    "            if geo_info:\n",
    "                geo_info['count'] = count\n",
    "                geocoded_locations[location_name] = geo_info\n",
    "            time.sleep(1)  # Be nice to the open geocoding service :)\n",
    "    \n",
    "    return geocoded_locations, df\n",
    "\n",
    "def create_folium_heatmap(geocoded_locations, output_file=\"crisis_heatmap.html\"):\n",
    "    \"\"\"Create a Folium heatmap of crisis locations\"\"\"\n",
    "    # Create a list of [lat, lon, weight] for heatmap\n",
    "    heatmap_data = []\n",
    "    for loc in geocoded_locations.values():\n",
    "        # Use count as weight\n",
    "        for _ in range(loc['count']):\n",
    "            random_lat = loc['lat'] + np.random.normal(0, 0.01)\n",
    "            random_lon = loc['lon'] + np.random.normal(0, 0.01)\n",
    "            heatmap_data.append([random_lat, random_lon, 1])\n",
    "    \n",
    "    # Create base map centered on the USA\n",
    "    m = folium.Map(location=[39.8283, -98.5795], zoom_start=4)\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    HeatMap(heatmap_data).add_to(m)\n",
    "    \n",
    "    # Add markers for top 5 locations\n",
    "    top_5_locations = sorted(geocoded_locations.values(), key=lambda x: x['count'], reverse=True)[:5]\n",
    "    \n",
    "    for loc in top_5_locations:\n",
    "        folium.Marker(\n",
    "            location=[loc['lat'], loc['lon']],\n",
    "            popup=f\"{loc['location']}: {loc['count']} mentions\",\n",
    "            tooltip=loc['location']\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save map\n",
    "    m.save(output_file)\n",
    "    print(f\"Folium heatmap saved to {output_file}\")\n",
    "    \n",
    "    return top_5_locations\n",
    "\n",
    "def create_plotly_heatmap(geocoded_locations, df, output_file=\"crisis_plotly_map.html\"):\n",
    "    \"\"\"Create a Plotly choropleth map of crisis locations\"\"\"\n",
    "    # Create a dataframe for plotting\n",
    "    top_locations_df = pd.DataFrame([\n",
    "        {\n",
    "            'location': loc['location'],\n",
    "            'lat': loc['lat'],\n",
    "            'lon': loc['lon'],\n",
    "            'count': loc['count']\n",
    "        } for loc in geocoded_locations.values()\n",
    "    ])\n",
    "    \n",
    "    # Create scatter mapbox\n",
    "    fig = px.scatter_mapbox(\n",
    "        top_locations_df,\n",
    "        lat=\"lat\",\n",
    "        lon=\"lon\",\n",
    "        size=\"count\",\n",
    "        color=\"count\",\n",
    "        hover_name=\"location\",\n",
    "        size_max=25,\n",
    "        zoom=3,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title=\"Crisis Mentions by Location\",\n",
    "        color_continuous_scale=px.colors.sequential.Inferno,\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        margin={\"r\": 0, \"t\": 30, \"l\": 0, \"b\": 0},\n",
    "        coloraxis_colorbar=dict(title=\"Post Count\")\n",
    "    )\n",
    "    \n",
    "    # Save to HTML\n",
    "    fig.write_html(output_file)\n",
    "    print(f\"Plotly visualization saved to {output_file}\")\n",
    "    \n",
    "    return top_locations_df\n",
    "\n",
    "def analyze_crisis_by_location(df, geocoded_locations):\n",
    "    \"\"\"Analyze crisis patterns by location\"\"\"\n",
    "    # Create a mapping of each post to its detected locations\n",
    "    post_locations = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        for loc in row['extracted_locations']:\n",
    "            if loc in geocoded_locations:\n",
    "                if idx not in post_locations:\n",
    "                    post_locations[idx] = []\n",
    "                post_locations[idx].append(loc)\n",
    "    \n",
    "    # For posts with locations, analyze risk level patterns\n",
    "    location_risk_levels = {}\n",
    "    for idx, locations in post_locations.items():\n",
    "        risk_level = df.loc[idx, 'risk_level']\n",
    "        for loc in locations:\n",
    "            if loc not in location_risk_levels:\n",
    "                location_risk_levels[loc] = []\n",
    "            location_risk_levels[loc].append(risk_level)\n",
    "    \n",
    "    # Calculate average risk level by location\n",
    "    location_avg_risk = {}\n",
    "    for loc, risks in location_risk_levels.items():\n",
    "        # Convert any risk levels to numeric if they're not already\n",
    "        numeric_risks = []\n",
    "        for risk in risks:\n",
    "            if isinstance(risk, (int, float)):\n",
    "                numeric_risks.append(risk)\n",
    "            elif isinstance(risk, str) and risk.replace('.', '', 1).isdigit():\n",
    "                numeric_risks.append(float(risk))\n",
    "        \n",
    "        if numeric_risks:  # Only calculate if we have valid numeric risk values\n",
    "            location_avg_risk[loc] = sum(numeric_risks) / len(numeric_risks)\n",
    "    \n",
    "    # Sort and return top locations by average risk\n",
    "    sorted_locations = sorted(location_avg_risk.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_locations\n",
    "\n",
    "def main(data_file_path):\n",
    "    \"\"\"Main function to process data and generate visualizations\"\"\"\n",
    "    # Load the dataset\n",
    "    print(f\"Loading data from {data_file_path}\")\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(df)} posts\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Process dataset to extract and geocode locations\n",
    "    geocoded_locations, processed_df = process_dataset(df)\n",
    "    \n",
    "    # Create heatmap visualizations\n",
    "    top_5_locations = create_folium_heatmap(geocoded_locations, output_file=\"plots_png/crisis_heatmap_nlp_geoExtracted.html\")\n",
    "    top_locations_df = create_plotly_heatmap(geocoded_locations, processed_df, output_file=\"crisis_plotly_map_unbiased.html\")\n",
    "    \n",
    "    # Analyze crisis patterns by location\n",
    "    location_risk_analysis = analyze_crisis_by_location(processed_df, geocoded_locations)\n",
    "    \n",
    "    # Print top 5 locations with highest crisis discussions\n",
    "    print(\"\\nTop 5 locations with highest crisis discussions:\")\n",
    "    for i, (loc_name, loc_data) in enumerate(sorted(\n",
    "            geocoded_locations.items(), \n",
    "            key=lambda x: x[1]['count'], \n",
    "            reverse=True\n",
    "        )[:5]):\n",
    "        print(f\"{i+1}. {loc_name}: {loc_data['count']} mentions\")\n",
    "    \n",
    "    # Print top 5 locations with highest average risk level\n",
    "    if location_risk_analysis:\n",
    "        print(\"\\nTop 5 locations with highest average risk level:\")\n",
    "        for i, (loc, avg_risk) in enumerate(location_risk_analysis[:5]):\n",
    "            print(f\"{i+1}. {loc}: {avg_risk:.2f} average risk\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(\"- Crisis heatmap saved as 'crisis_heatmap_unbiased.html'\")\n",
    "    print(\"- Plotly visualization saved as 'crisis_plotly_map_unbiased.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from mental_health_postsV1_classified.csv\n",
      "Dataset loaded with 104483 posts\n",
      "Columns: ['post_id', 'timestamp', 'title', 'content', 'subreddit', 'ups', 'num_comments', 'url', 'risk_level']\n",
      "Extracting locations from posts...\n",
      "Top 10 most mentioned locations: [('uk', 977), ('us', 394), ('canada', 318), ('california', 296), ('va', 286), ('europe', 239), ('nc', 230), ('UK', 222), ('emdr', 219), ('florida', 210)]\n",
      "Geocoding top locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
      "/tmp/ipykernel_1754/780046504.py:150: DeprecationWarning:\n",
      "\n",
      "*scatter_mapbox* is deprecated! Use *scatter_map* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folium heatmap saved to crisis_heatmap_unbiased.html\n",
      "Plotly visualization saved to crisis_plotly_map_unbiased.html\n",
      "\n",
      "Top 5 locations with highest crisis discussions:\n",
      "1. uk: 977 mentions\n",
      "2. us: 394 mentions\n",
      "3. canada: 318 mentions\n",
      "4. california: 296 mentions\n",
      "5. va: 286 mentions\n",
      "\n",
      "Analysis complete!\n",
      "- Crisis heatmap saved as 'crisis_heatmap.html'\n",
      "- Plotly visualization saved as 'crisis_plotly_map.html'\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to run the above process\n",
    "# main(data_file_path='mental_health_postsV1_classified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>text_for_location</th>\n",
       "      <th>extracted_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ek7px9</td>\n",
       "      <td>2024-08-05T03:54:23</td>\n",
       "      <td>I want to go off my meds for my wedding</td>\n",
       "      <td>title pretty much covers getting married next ...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1ek...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>I want to go off my meds for my wedding title ...</td>\n",
       "      <td>vega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ek7px9</td>\n",
       "      <td>2024-08-05T03:54:23</td>\n",
       "      <td>I want to go off my meds for my wedding</td>\n",
       "      <td>title pretty much covers getting married next ...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1ek...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>I want to go off my meds for my wedding title ...</td>\n",
       "      <td>vega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1att0dy</td>\n",
       "      <td>2024-02-18T17:57:37</td>\n",
       "      <td>New to Abilify</td>\n",
       "      <td>hi started abilify im already lithium lamictal...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1at...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>New to Abilify hi started abilify im already l...</td>\n",
       "      <td>home depot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1869tyv</td>\n",
       "      <td>2023-11-29T04:38:19</td>\n",
       "      <td>I was diagnosed bipolar 1 now year later I’m g...</td>\n",
       "      <td>uhm anyone else feel like world inside glass c...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/186...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>I was diagnosed bipolar 1 now year later I’m g...</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17rceyh</td>\n",
       "      <td>2023-11-09T18:30:56</td>\n",
       "      <td>Feeling isolated from my disorders</td>\n",
       "      <td>feel like theres no one understands im going e...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/17r...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>Feeling isolated from my disorders feel like t...</td>\n",
       "      <td>psychiatric hospital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15859</th>\n",
       "      <td>1ieoubh</td>\n",
       "      <td>2025-02-01T02:15:26</td>\n",
       "      <td>Destroyed my dating life with manic tattoos</td>\n",
       "      <td>destroyed dating life horrible manic tattoos p...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.reddit.com/gallery/1ieoubh</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>Destroyed my dating life with manic tattoos de...</td>\n",
       "      <td>switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15862</th>\n",
       "      <td>1gkk70l</td>\n",
       "      <td>2024-11-06T04:40:38</td>\n",
       "      <td>never ending depression.</td>\n",
       "      <td>feel alone many good things appreciative grate...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1gk...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>never ending depression.  feel alone many good...</td>\n",
       "      <td>baseball stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15863</th>\n",
       "      <td>1gffyt9</td>\n",
       "      <td>2024-10-30T11:39:12</td>\n",
       "      <td>BUSPAR</td>\n",
       "      <td>anyone prescribed buspar bipolar related anxie...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1gf...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>BUSPAR  anyone prescribed buspar bipolar relat...</td>\n",
       "      <td>BUSPAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15864</th>\n",
       "      <td>1fjt5qc</td>\n",
       "      <td>2024-09-18T19:14:13</td>\n",
       "      <td>My bipolar symptoms History based on what I ca...</td>\n",
       "      <td>first manic episode happened grade final year ...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1fj...</td>\n",
       "      <td>Low Concern</td>\n",
       "      <td>My bipolar symptoms History based on what I ca...</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15865</th>\n",
       "      <td>1fjma7l</td>\n",
       "      <td>2024-09-18T11:56:30</td>\n",
       "      <td>Need to vent! Looking for friendly advice from...</td>\n",
       "      <td>good morning ive introduced subreddit name jes...</td>\n",
       "      <td>bipolar1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/bipolar1/comments/1fj...</td>\n",
       "      <td>Moderate Concern</td>\n",
       "      <td>Need to vent! Looking for friendly advice from...</td>\n",
       "      <td>psych</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13272 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id            timestamp  \\\n",
       "0      1ek7px9  2024-08-05T03:54:23   \n",
       "1      1ek7px9  2024-08-05T03:54:23   \n",
       "3      1att0dy  2024-02-18T17:57:37   \n",
       "4      1869tyv  2023-11-29T04:38:19   \n",
       "5      17rceyh  2023-11-09T18:30:56   \n",
       "...        ...                  ...   \n",
       "15859  1ieoubh  2025-02-01T02:15:26   \n",
       "15862  1gkk70l  2024-11-06T04:40:38   \n",
       "15863  1gffyt9  2024-10-30T11:39:12   \n",
       "15864  1fjt5qc  2024-09-18T19:14:13   \n",
       "15865  1fjma7l  2024-09-18T11:56:30   \n",
       "\n",
       "                                                   title  \\\n",
       "0                I want to go off my meds for my wedding   \n",
       "1                I want to go off my meds for my wedding   \n",
       "3                                         New to Abilify   \n",
       "4      I was diagnosed bipolar 1 now year later I’m g...   \n",
       "5                     Feeling isolated from my disorders   \n",
       "...                                                  ...   \n",
       "15859        Destroyed my dating life with manic tattoos   \n",
       "15862                          never ending depression.    \n",
       "15863                                            BUSPAR    \n",
       "15864  My bipolar symptoms History based on what I ca...   \n",
       "15865  Need to vent! Looking for friendly advice from...   \n",
       "\n",
       "                                                 content subreddit  ups  \\\n",
       "0      title pretty much covers getting married next ...  bipolar1    0   \n",
       "1      title pretty much covers getting married next ...  bipolar1    0   \n",
       "3      hi started abilify im already lithium lamictal...  bipolar1    4   \n",
       "4      uhm anyone else feel like world inside glass c...  bipolar1    2   \n",
       "5      feel like theres no one understands im going e...  bipolar1    1   \n",
       "...                                                  ...       ...  ...   \n",
       "15859  destroyed dating life horrible manic tattoos p...  bipolar1    7   \n",
       "15862  feel alone many good things appreciative grate...  bipolar1    8   \n",
       "15863  anyone prescribed buspar bipolar related anxie...  bipolar1    2   \n",
       "15864  first manic episode happened grade final year ...  bipolar1    5   \n",
       "15865  good morning ive introduced subreddit name jes...  bipolar1    9   \n",
       "\n",
       "       num_comments                                                url  \\\n",
       "0                12  https://www.reddit.com/r/bipolar1/comments/1ek...   \n",
       "1                12  https://www.reddit.com/r/bipolar1/comments/1ek...   \n",
       "3                 2  https://www.reddit.com/r/bipolar1/comments/1at...   \n",
       "4                 1  https://www.reddit.com/r/bipolar1/comments/186...   \n",
       "5                 5  https://www.reddit.com/r/bipolar1/comments/17r...   \n",
       "...             ...                                                ...   \n",
       "15859            14             https://www.reddit.com/gallery/1ieoubh   \n",
       "15862             4  https://www.reddit.com/r/bipolar1/comments/1gk...   \n",
       "15863             8  https://www.reddit.com/r/bipolar1/comments/1gf...   \n",
       "15864             0  https://www.reddit.com/r/bipolar1/comments/1fj...   \n",
       "15865             6  https://www.reddit.com/r/bipolar1/comments/1fj...   \n",
       "\n",
       "             risk_level                                  text_for_location  \\\n",
       "0      Moderate Concern  I want to go off my meds for my wedding title ...   \n",
       "1      Moderate Concern  I want to go off my meds for my wedding title ...   \n",
       "3      Moderate Concern  New to Abilify hi started abilify im already l...   \n",
       "4      Moderate Concern  I was diagnosed bipolar 1 now year later I’m g...   \n",
       "5      Moderate Concern  Feeling isolated from my disorders feel like t...   \n",
       "...                 ...                                                ...   \n",
       "15859  Moderate Concern  Destroyed my dating life with manic tattoos de...   \n",
       "15862  Moderate Concern  never ending depression.  feel alone many good...   \n",
       "15863  Moderate Concern  BUSPAR  anyone prescribed buspar bipolar relat...   \n",
       "15864       Low Concern  My bipolar symptoms History based on what I ca...   \n",
       "15865  Moderate Concern  Need to vent! Looking for friendly advice from...   \n",
       "\n",
       "         extracted_location  \n",
       "0                      vega  \n",
       "1                      vega  \n",
       "3                home depot  \n",
       "4                california  \n",
       "5      psychiatric hospital  \n",
       "...                     ...  \n",
       "15859           switzerland  \n",
       "15862      baseball stadium  \n",
       "15863                BUSPAR  \n",
       "15864               america  \n",
       "15865                 psych  \n",
       "\n",
       "[13272 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['extracted_location'].str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-2\n",
    "\n",
    "using roberta-large-ner-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/geographic_extraction_with_roberta.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/geographic_extraction_with_roberta.py\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "# Create NER pipeline on specified device\n",
    "def create_ner_pipeline(device_id=None):\n",
    "    device = f\"cuda:{device_id}\" if device_id is not None else \"cpu\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "def extract_locations_with_transformer(text, ner_pipeline):\n",
    "    \"\"\"\n",
    "    Use a transformer-based NER model to identify location mentions in text.\n",
    "    Returns a list of location names found in the text.\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text) or text.strip() == \"\":\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Extract named entities\n",
    "        results = ner_pipeline(text)\n",
    "        # Filter for location entities (LOC or GPE)\n",
    "        locations = []\n",
    "        for entity in results:\n",
    "            if entity['entity_group'] in ['LOC', 'GPE']:\n",
    "                # Clean up the location string\n",
    "                location = entity['word'].strip()\n",
    "                if location:\n",
    "                    locations.append(location)\n",
    "        return locations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in NER extraction: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_batch(batch_df, device_id=None):\n",
    "    # Create pipeline on appropriate device\n",
    "    nlp = create_ner_pipeline(device_id)\n",
    "    \n",
    "    batch_results = []\n",
    "    \n",
    "    for _, row in batch_df.iterrows():\n",
    "        # Extract locations from the combined text\n",
    "        locations = extract_locations_with_transformer(row['text_for_location'], nlp)\n",
    "        \n",
    "        # Add each location as a separate entry with all original row data\n",
    "        for loc in locations:\n",
    "            # Create a copy of the original row\n",
    "            result_row = row.copy()\n",
    "            # Add the location\n",
    "            result_row['extracted_location'] = loc\n",
    "            # Add to results\n",
    "            batch_results.append(result_row)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# Function to geocode locations using a geocoding API\n",
    "def geocode_location(location_name):\n",
    "    \"\"\"\n",
    "    Convert location name to latitude and longitude using a geocoding API.\n",
    "    Returns (lat, lng) tuple or None if geocoding fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_url = \"https://nominatim.openstreetmap.org/search\"\n",
    "        params = {\n",
    "            \"q\": location_name,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": 1\n",
    "        }\n",
    "        headers = {\n",
    "            \"User-Agent\": \"CrisisLocationAnalysis/1.0\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            if results:\n",
    "                lat = float(results[0][\"lat\"])\n",
    "                lon = float(results[0][\"lon\"])\n",
    "                return (lat, lon)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Geocoding error for {location_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def map_risk_level_to_numeric(risk_text):\n",
    "    # Define mapping of text values to numeric scores\n",
    "    risk_mapping = {\n",
    "        'Low Concern': 1.0,\n",
    "        'Moderate Concern': 2.5,\n",
    "        'High-Risk': 5.0,\n",
    "        'Unclassified': 0.0  # Assign a small value for unclassified\n",
    "    }\n",
    "    \n",
    "    # Return the mapped value or a default if not found\n",
    "    return risk_mapping.get(risk_text, 0.5)\n",
    "    \n",
    "def main():\n",
    "    # Load the dataset\n",
    "    file_path = \"mental_health_postsV1_classified.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Combine title and content for better location extraction\n",
    "    df['text_for_location'] = df['title'].fillna('') + ' ' + df['content'].fillna('')\n",
    "    \n",
    "    # Check for GPU availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    num_gpus = torch.cuda.device_count() if gpu_available else 0\n",
    "    \n",
    "    if gpu_available:\n",
    "        print(f\"GPU processing enabled! Found {num_gpus} GPU devices.\")\n",
    "        device_type = \"GPU\"\n",
    "    else:\n",
    "        print(\"No GPUs found. Using CPU processing.\")\n",
    "        device_type = \"CPU\"\n",
    "        num_gpus = 0\n",
    "    \n",
    "    # Determine workers based on available hardware\n",
    "    if gpu_available:\n",
    "        # Use GPU processing - create one batch per GPU\n",
    "        num_workers = num_gpus\n",
    "    else:\n",
    "        # Use CPU processing - create batches based on CPU cores\n",
    "        num_workers = os.cpu_count() or 4\n",
    "    \n",
    "    # Create batches\n",
    "    batch_size = max(1, len(df) // max(1, num_workers))\n",
    "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    print(f\"Using {num_workers} {device_type} workers with batch size {batch_size}\")\n",
    "    \n",
    "    # Extract locations using parallel processing\n",
    "    print(\"Extracting locations using parallel processing...\")\n",
    "    all_results = []\n",
    "    \n",
    "    if gpu_available:\n",
    "        # GPU processing using ThreadPoolExecutor\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_gpus) as executor:\n",
    "            futures = []\n",
    "            for i, batch in enumerate(batches):\n",
    "                # Assign each batch to a GPU (cycling if more batches than GPUs)\n",
    "                gpu_id = i % num_gpus\n",
    "                future = executor.submit(process_batch, batch, gpu_id)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "                try:\n",
    "                    batch_results = future.result()\n",
    "                    all_results.extend(batch_results)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch processing: {e}\")\n",
    "    else:\n",
    "        # CPU processing using ProcessPoolExecutor\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = []\n",
    "            for batch in batches:\n",
    "                future = executor.submit(process_batch, batch)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "                try:\n",
    "                    batch_results = future.result()\n",
    "                    all_results.extend(batch_results)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch processing: {e}\")\n",
    "    \n",
    "    # Create a dataframe from all results\n",
    "    if all_results:\n",
    "        locations_df = pd.DataFrame(all_results)\n",
    "    else:\n",
    "        print(\"No locations found!\")\n",
    "        return\n",
    "    \n",
    "    # Save the complete dataset with extracted locations\n",
    "    locations_df.to_csv(\"crisis_locations_extracted.csv\", index=False)\n",
    "    print(f\"Saved {len(locations_df)} entries with location data to crisis_locations_extracted.csv\")\n",
    "\n",
    "    # # Count occurrences of each location\n",
    "    location_counts = Counter(locations_df['extracted_location'])\n",
    "    \n",
    "    # Geocode each unique location\n",
    "    print(\"Geocoding unique locations...\")\n",
    "    geocoded_locations = {}\n",
    "    unique_locations = list(location_counts.keys())\n",
    "    \n",
    "    for loc in tqdm(unique_locations):\n",
    "        if loc not in geocoded_locations:\n",
    "            coords = geocode_location(loc)\n",
    "            if coords:\n",
    "                geocoded_locations[loc] = coords\n",
    "            time.sleep(1)  # Respect API rate limits\n",
    "    \n",
    "    # Add geocoded coordinates to the locations dataframe\n",
    "    locations_df['coordinates'] = locations_df['extracted_location'].map(lambda x: geocoded_locations.get(x, None))\n",
    "    locations_df = locations_df.dropna(subset=['coordinates'])\n",
    "    \n",
    "    # Extract latitude and longitude from coordinates\n",
    "    locations_df['latitude'] = locations_df['coordinates'].map(lambda x: x[0] if x else None)\n",
    "    locations_df['longitude'] = locations_df['coordinates'].map(lambda x: x[1] if x else None)\n",
    "    \n",
    "    # Save the geocoded data\n",
    "    locations_df.to_csv(\"crisis_locations_geocoded.csv\", index=False)\n",
    "    print(f\"Saved {len(locations_df)} geocoded entries to crisis_locations_geocoded.csv\")\n",
    "\n",
    "    # this is the data we saved two lines above\n",
    "    # locations_df = pd.read_csv(\"/kaggle/input/issr-reddit-locations-data/crisis_locations_geocoded.csv\")\n",
    "\n",
    "    # Calculate total risk per location (if risk_level exists in the dataset)\n",
    "    geocoded_locations = {}\n",
    "\n",
    "    # Iterate through unique locations in the dataframe\n",
    "    for _, row in locations_df.drop_duplicates(subset=['extracted_location']).iterrows():\n",
    "        if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n",
    "            # Store the coordinates as a tuple (lat, lng)\n",
    "            geocoded_locations[row['extracted_location']] = (row['latitude'], row['longitude'])\n",
    "    \n",
    "    print(f\"Built dictionary with {len(geocoded_locations)} unique locations\")\n",
    "    \n",
    "    if 'risk_level' in locations_df.columns:\n",
    "\n",
    "        print(\"Checking location names...\")\n",
    "        if any(loc.count('Unclassified') > 1 for loc in locations_df['extracted_location'].unique() if isinstance(loc, str)):\n",
    "            print(\"Detected corrupted location names, fixing...\")\n",
    "        \n",
    "            # Fix the corrupted location names\n",
    "            def clean_location_name(name):\n",
    "                if isinstance(name, str):\n",
    "                    if name.count('Unclassified') > 1:\n",
    "                        return 'Unclassified'\n",
    "                    if name.count('Moderate Concern') > 1:\n",
    "                        return 'Moderate Concern'\n",
    "                    if name.count('High-Risk') > 1:\n",
    "                        return 'High-Risk'\n",
    "                    if name.count('Low Concern') > 1:\n",
    "                        return 'Low Concern'\n",
    "                return name\n",
    "            \n",
    "            # Apply the cleaning function\n",
    "            locations_df['clean_location'] = locations_df['extracted_location'].apply(clean_location_name)\n",
    "            # Use the cleaned column for visualization\n",
    "            locations_df = locations_df.rename(columns={'extracted_location': 'original_location', 'clean_location': 'extracted_location'})\n",
    "            \n",
    "            # Update the geocoded_locations dictionary with the cleaned names\n",
    "            new_geocoded_locations = {}\n",
    "            for loc, coords in geocoded_locations.items():\n",
    "                clean_loc = clean_location_name(loc)\n",
    "                new_geocoded_locations[clean_loc] = coords\n",
    "            geocoded_locations = new_geocoded_locations\n",
    "\n",
    "        # Prepare data for heatmap\n",
    "        locations_df['numeric_risk'] = locations_df['risk_level'].apply(map_risk_level_to_numeric)\n",
    "        \n",
    "        location_risk = locations_df.groupby('extracted_location')['numeric_risk'].sum().reset_index()\n",
    "        location_risk = location_risk.sort_values('numeric_risk', ascending=False)\n",
    "        \n",
    "        # Get top 5 locations with highest crisis discussions\n",
    "        top_5_locations = location_risk.head(5)\n",
    "        print(\"\\nTop 5 locations with highest crisis discussions:\")\n",
    "        print(top_5_locations)\n",
    "        \n",
    "        # Create heatmap using Folium\n",
    "        print(\"Creating heatmap visualization...\")\n",
    "        m = folium.Map(location=[39.50, -98.35], zoom_start=4)\n",
    "        \n",
    "        # Prepare data for heatmap - ensure all values are numeric\n",
    "        heat_data = []\n",
    "        for _, row in locations_df.iterrows():\n",
    "            try:\n",
    "                lat = float(row['latitude'])\n",
    "                lng = float(row['longitude'])\n",
    "                risk = float(row['numeric_risk'])\n",
    "                \n",
    "                # Only add if all values are valid numbers\n",
    "                if not (pd.isna(lat) or pd.isna(lng) or pd.isna(risk)):\n",
    "                    heat_data.append([lat, lng, risk])\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        # Add heatmap to the map if we have valid data\n",
    "        if heat_data:\n",
    "            import numpy as np\n",
    "            heat_data_array = np.array(heat_data)\n",
    "            HeatMap(heat_data_array, radius=15).add_to(m)\n",
    "        else:\n",
    "            print(\"Warning: No valid data for heatmap\")\n",
    "        \n",
    "        # Add markers for top 5 locations with cleaner labels\n",
    "        for _, row in top_5_locations.iterrows():\n",
    "            if row['extracted_location'] in geocoded_locations:\n",
    "                coords = geocoded_locations[row['extracted_location']]\n",
    "                # Get the original risk text for this location (first occurrence)\n",
    "                original_risk = locations_df[locations_df['extracted_location'] == row['extracted_location']]['risk_level'].iloc[0]\n",
    "                \n",
    "                # Create a cleaner popup\n",
    "                popup_html = f\"\"\"\n",
    "                <div style=\"font-family: Arial, sans-serif; padding: 5px;\">\n",
    "                    <h4>{row['extracted_location']}</h4>\n",
    "                    <p>Risk Level: {original_risk}</p>\n",
    "                    <p>Risk Score: {row['numeric_risk']:.1f}</p>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "                \n",
    "                folium.Marker(\n",
    "                    location=coords,\n",
    "                    popup=folium.Popup(popup_html, max_width=300),\n",
    "                    tooltip=row['extracted_location'],\n",
    "                    icon=folium.Icon(color='red', icon='info-sign')\n",
    "                ).add_to(m)\n",
    "        \n",
    "        m.save('crisis_heatmap.html')\n",
    "        \n",
    "        # Create a Plotly visualization for top locations\n",
    "        fig = px.bar(\n",
    "            top_5_locations,\n",
    "            x='extracted_location',\n",
    "            y='numeric_risk',\n",
    "            title='Top 5 Locations with Highest Crisis Discussions',\n",
    "            labels={'extracted_location': 'Location', 'numeric_risk': 'Crisis Risk Score'},\n",
    "            text='numeric_risk'\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Location\",\n",
    "            yaxis_title=\"Crisis Risk Score\",\n",
    "            xaxis={'categoryorder': 'total descending'},  # Sort by highest risk\n",
    "            font=dict(size=12),\n",
    "            xaxis_tickangle=-45,\n",
    "            margin=dict(l=50, r=50, t=80, b=100)\n",
    "        )\n",
    "        \n",
    "        # Format the bar text\n",
    "        fig.update_traces(\n",
    "            texttemplate='%{text:.1f}',\n",
    "            textposition='outside'\n",
    "        )\n",
    "        \n",
    "        fig.write_html('top_locations.html')\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Files saved:\")\n",
    "    print(\"- crisis_locations_extracted.csv (all records with location data)\")\n",
    "    print(\"- crisis_locations_geocoded.csv (records with geocoded locations)\")\n",
    "    if 'risk_level' in locations_df.columns:\n",
    "        print(\"- crisis_heatmap.html (interactive map)\")\n",
    "        print(\"- top_locations.html (bar chart of top locations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- robert-extracted more locations in 16K posts, using SpaCy : 10K posts\n",
    "- GeoCoded coords are visually flawed in Method-2\n",
    "- openstreetmap's lack proper semantic search, GoogleMapsAPI can be used to overcome this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this and then cell to extract locations\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more Analysis and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\"\"\"This script generates various visualizations for the crisis terms dataset: crisis_terms_bert.csv.\n",
    "We are analyzing the risk levels and sentiment scores over time, as well as the distribution of these metrics.\n",
    "\n",
    "Consclusion infered via plots:\n",
    " - Their is more data for the recent year, so we can't infer that high-risk posts are increasing over year.\n",
    " - The sentiment score is not a good indicator of risk level. Needs to study their methods relevance.\n",
    " - There is a clear seasonal pattern in the risk levels, with slightly more high-risk-levels in later second-half of year.\"\"\"\n",
    "try:\n",
    "    df = pd.read_csv(\"crisis_terms_bert.csv\") # Provide proper path to the dataset\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Extract year and month for analysis\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['year_month'] = df['timestamp'].dt.strftime('%Y-%m')\n",
    "\n",
    "# 1. Monthly Average Risk Level Analysis\n",
    "# Convert risk_level to numeric for averaging\n",
    "risk_map = {'Low Concern': 1, 'Moderate Concern': 2, 'High-Risk': 3, 'Unclassified': 0}\n",
    "df['risk_numeric'] = df['risk_level'].map(risk_map)\n",
    "\n",
    "# Group by year and month\n",
    "monthly_risk = df.groupby('year_month').agg({\n",
    "    'risk_numeric': 'mean',\n",
    "    'sentiment_score': 'mean',\n",
    "    'timestamp': 'first'  # Keep first timestamp for proper ordering\n",
    "}).reset_index()\n",
    "\n",
    "# Sort by timestamp\n",
    "monthly_risk = monthly_risk.sort_values('timestamp')\n",
    "\n",
    "# 2. Risk Level Distribution by Month and Year\n",
    "risk_counts_monthly = df.groupby(['year_month', 'risk_level']).size().reset_index(name='count')\n",
    "risk_counts_yearly = df.groupby(['year', 'risk_level']).size().reset_index(name='count')\n",
    "\n",
    "# 3. Sentiment Analysis Over Time\n",
    "monthly_sentiment = df.groupby('year_month').agg({\n",
    "    'sentiment_score': 'mean',\n",
    "    'timestamp': 'first'\n",
    "}).reset_index().sort_values('timestamp')\n",
    "\n",
    "sentiment_counts_monthly = df.groupby(['year_month', 'sentiment']).size().reset_index(name='count')\n",
    "sentiment_counts_yearly = df.groupby(['year', 'sentiment']).size().reset_index(name='count')\n",
    "\n",
    "# 4. Combined Risk and Sentiment Analysis\n",
    "combined_monthly = df.groupby('year_month').agg({\n",
    "    'risk_numeric': 'mean',\n",
    "    'sentiment_score': 'mean',\n",
    "    'timestamp': 'first'\n",
    "}).reset_index().sort_values('timestamp')\n",
    "\n",
    "# 5. High Risk Terms Analysis\n",
    "df['has_high_risk_terms'] = df['high_risk_terms'].apply(lambda x: 1 if len(x) > 0 else 0)\n",
    "high_risk_monthly = df.groupby('year_month').agg({\n",
    "    'has_high_risk_terms': 'sum',\n",
    "    'timestamp': 'first'\n",
    "}).reset_index().sort_values('timestamp')\n",
    "\n",
    "# Create visualizations\n",
    "\n",
    "# 1. Monthly Average Risk Level Trend\n",
    "fig1 = px.line(monthly_risk, x='year_month', y='risk_numeric', \n",
    "              title='Average Risk Level Over Time',\n",
    "              labels={'risk_numeric': 'Risk Level (1=Low, 4=Critical)', 'year_month': 'Month'})\n",
    "\n",
    "fig1.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# 2. Risk Level Distribution Stacked Area Chart\n",
    "fig2 = px.area(risk_counts_monthly, x='year_month', y='count', color='risk_level',\n",
    "              title='Risk Level Distribution by Month',\n",
    "              labels={'count': 'Number of Posts', 'year_month': 'Month', 'risk_level': 'Risk Level'},\n",
    "              color_discrete_map={'low': 'green', 'medium': 'yellow', 'high': 'orange', 'critical': 'red'})\n",
    "\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# 3. Sentiment Score Trend\n",
    "fig3 = px.line(monthly_sentiment, x='year_month', y='sentiment_score',\n",
    "              title='Average Sentiment Score Over Time',\n",
    "              labels={'sentiment_score': 'Sentiment Score (-1 to 1)', 'year_month': 'Month'})\n",
    "\n",
    "fig3.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# 4. Sentiment Distribution\n",
    "fig4 = px.area(sentiment_counts_monthly, x='year_month', y='count', color='sentiment',\n",
    "              title='Sentiment Distribution by Month',\n",
    "              labels={'count': 'Number of Posts', 'year_month': 'Month', 'sentiment': 'Sentiment'},\n",
    "              color_discrete_map={'negative': 'red', 'neutral': 'gray', 'positive': 'green'})\n",
    "\n",
    "fig4.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# 5. Combined Risk and Sentiment by Month\n",
    "fig5 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig5.add_trace(\n",
    "    go.Scatter(x=combined_monthly['year_month'], y=combined_monthly['risk_numeric'], name=\"Risk Level\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig5.add_trace(\n",
    "    go.Scatter(x=combined_monthly['year_month'], y=combined_monthly['sentiment_score'], name=\"Sentiment Score\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig5.update_layout(\n",
    "    title_text=\"Risk Level vs Sentiment Score Over Time\",\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "\n",
    "fig5.update_yaxes(title_text=\"Risk Level (1-4)\", secondary_y=False)\n",
    "fig5.update_yaxes(title_text=\"Sentiment Score (-1 to 1)\", secondary_y=True)\n",
    "\n",
    "# 6. Yearly Risk Level Distribution\n",
    "fig6 = px.bar(risk_counts_yearly, x='year', y='count', color='risk_level', barmode='group',\n",
    "             title='Risk Level Distribution by Year',\n",
    "             labels={'count': 'Number of Posts', 'year': 'Year', 'risk_level': 'Risk Level'},\n",
    "             color_discrete_map={'low': 'green', 'medium': 'yellow', 'high': 'orange', 'critical': 'red'})\n",
    "\n",
    "# 7. Yearly Sentiment Distribution\n",
    "fig7 = px.bar(sentiment_counts_yearly, x='year', y='count', color='sentiment', barmode='group',\n",
    "             title='Sentiment Distribution by Year',\n",
    "             labels={'count': 'Number of Posts', 'year': 'Year', 'sentiment': 'Sentiment'},\n",
    "             color_discrete_map={'negative': 'red', 'neutral': 'gray', 'positive': 'green'})\n",
    "\n",
    "# 8. High Risk Terms Occurrence Over Time\n",
    "fig8 = px.bar(high_risk_monthly, x='year_month', y='has_high_risk_terms',\n",
    "             title='Posts with High Risk Terms Over Time',\n",
    "             labels={'has_high_risk_terms': 'Number of Posts', 'year_month': 'Month'})\n",
    "\n",
    "fig8.update_layout(xaxis_tickangle=-45)\n",
    "\n",
    "# Display the figures\n",
    "# fig1.show()\n",
    "# fig2.show()\n",
    "# fig3.show()\n",
    "# fig4.show()\n",
    "# fig5.show()\n",
    "# fig6.show()\n",
    "# fig7.show()\n",
    "# fig8.show()\n",
    "\n",
    "# Create a dashboard layout\n",
    "dashboard_fig = make_subplots(\n",
    "    rows=4, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Average Risk Level Over Time\",\n",
    "        \"Risk Level Distribution by Month\",\n",
    "        \"Average Sentiment Score Over Time\",\n",
    "        \"Sentiment Distribution by Month\",\n",
    "        \"Risk Level vs Sentiment Score Over Time\",\n",
    "        \"Risk Level Distribution by Year\",\n",
    "        \"Sentiment Distribution by Year\",\n",
    "        \"Posts with High Risk Terms Over Time\"\n",
    "    ),\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Add each figure as a subplot\n",
    "dashboard_fig.add_traces(fig1.data, rows=1, cols=1)\n",
    "dashboard_fig.add_traces(fig2.data, rows=1, cols=2)\n",
    "dashboard_fig.add_traces(fig3.data, rows=2, cols=1)\n",
    "dashboard_fig.add_traces(fig4.data, rows=2, cols=2)\n",
    "dashboard_fig.add_traces(fig5.data, rows=3, cols=1)\n",
    "dashboard_fig.add_traces(fig6.data, rows=3, cols=2)\n",
    "dashboard_fig.add_traces(fig7.data, rows=4, cols=1)\n",
    "dashboard_fig.add_traces(fig8.data, rows=4, cols=2)\n",
    "\n",
    "# Update layout for better visualization\n",
    "dashboard_fig.update_layout(\n",
    "    height=1200, width=1800,\n",
    "    title_text=\"Dashboard: Risk and Sentiment Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Display the dashboard\n",
    "dashboard_fig.write_image(\"dashboard.png\", scale=2)\n",
    "dashboard_fig.write_html(\"dashboard.html\", include_plotlyjs='cdn')\n",
    "\n",
    "def create_risk_heatmap(df):\n",
    "    \n",
    "    # Ensure risk_level is properly converted to numeric\n",
    "    if df['risk_level'].dtype == 'object':\n",
    "        risk_map = {'Low Concern': 1, 'Moderate Concern': 2, 'High-Risk': 3, 'Unclassified': 0}\n",
    "        df['risk_numeric'] = df['risk_level'].map(risk_map)\n",
    "\n",
    "    # Extract numeric month for proper ordering\n",
    "    df['month_num'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Create pivot table with proper ordering\n",
    "    heatmap_data = df.pivot_table(\n",
    "        values='risk_numeric', \n",
    "        index='month_num',  # Use numeric month for correct ordering\n",
    "        columns='year', \n",
    "        aggfunc='mean'\n",
    "    ).round(2)\n",
    "\n",
    "    # Create month labels for y-axis\n",
    "    month_names = {\n",
    "        1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "        7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
    "    }\n",
    "\n",
    "    # Sort by month number to ensure correct order\n",
    "    heatmap_data = heatmap_data.sort_index()\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig_heatmap = px.imshow(\n",
    "        heatmap_data,\n",
    "        labels=dict(x=\"Year\", y=\"Month\", color=\"Avg Risk Level\"),\n",
    "        x=heatmap_data.columns,\n",
    "        y=[month_names[i] for i in heatmap_data.index],\n",
    "        title=\"Monthly Risk Level Heatmap\",\n",
    "        color_continuous_scale=\"RdYlGn_r\"\n",
    "    )\n",
    "\n",
    "    # Add custom layout\n",
    "    fig_heatmap.update_layout(\n",
    "        height=600,\n",
    "        width=800,\n",
    "        xaxis={'side': 'bottom'},\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Month\"\n",
    "    )\n",
    "\n",
    "    # Fix axis display issues\n",
    "    fig_heatmap.update_xaxes(tickangle=0)\n",
    "\n",
    "    fig_heatmap.write_image(\"monthly_risk_heatmap.png\", scale=2)\n",
    "    fig_heatmap.write_html(\"monthly_risk_heatmap.html\", include_plotlyjs='cdn')\n",
    "\n",
    "    return fig_heatmap\n",
    "\n",
    "def seasonal_analysis(df):\n",
    "    if df['risk_level'].dtype == 'object':\n",
    "        risk_map = {'Low Concern': 1, 'Moderate Concern': 2, 'High-Risk': 3, 'Unclassified': 0}\n",
    "        df['risk_numeric'] = df['risk_level'].map(risk_map)\n",
    "\n",
    "    # Add season column with better ordering\n",
    "    season_order = {'Winter': 1, 'Spring': 2, 'Summer': 3, 'Fall': 4}\n",
    "    df['season'] = df['timestamp'].dt.month.map({\n",
    "        1: 'Winter', 2: 'Winter', 3: 'Spring', \n",
    "        4: 'Spring', 5: 'Spring', 6: 'Summer',\n",
    "        7: 'Summer', 8: 'Summer', 9: 'Fall',\n",
    "        10: 'Fall', 11: 'Fall', 12: 'Winter'\n",
    "    })\n",
    "\n",
    "    seasonal_risk = df.groupby(['year', 'season']).agg({\n",
    "        'risk_numeric': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    fig_seasonal = px.line(\n",
    "        seasonal_risk, \n",
    "        x='season', \n",
    "        y='risk_numeric', \n",
    "        color='year',\n",
    "        title='Seasonal Risk Level Analysis',\n",
    "        labels={'risk_numeric': 'Average Risk Level', 'season': 'Season'},\n",
    "        category_orders={\"season\": [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]}\n",
    "    )\n",
    "\n",
    "    fig_seasonal.update_layout(\n",
    "        height=600,\n",
    "        width=900,\n",
    "        legend_title=\"Year\",\n",
    "        yaxis_range=[1, 4]\n",
    "    )\n",
    "\n",
    "    fig_seasonal.write_image(\"seasonal_risk_analysis.png\", scale=2)\n",
    "    fig_seasonal.write_html(\"seasonal_risk_analysis.html\", include_plotlyjs='cdn')\n",
    "\n",
    "    return fig_seasonal\n",
    "\n",
    "fig_seasonal = seasonal_analysis(df)\n",
    "fig_heatmap = create_risk_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
